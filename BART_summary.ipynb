{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMybRTlL7UOPsfWoQigZg9e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/youzhanghe123/A-simple-BART-summary-pipeline/blob/main/BART_summary.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Ybw3mFJwQeE",
        "outputId": "6979992a-74fa-4edc-c887-2bd4a369af52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from transformers import BartTokenizer\n",
        "from transformers import BartForConditionalGeneration\n",
        "from transformers import AdamW\n",
        "import torch\n",
        "from rouge_score import rouge_scorer"
      ],
      "metadata": {
        "id": "iS9r81gnPTpj"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path=\"/content/drive/MyDrive/Reviews.csv\""
      ],
      "metadata": {
        "id": "PukRQaCmLk6p"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(file_path).head(10)"
      ],
      "metadata": {
        "id": "XZNLcfvAL1W9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_eval=pd.read_csv(file_path).head(20)[10:]"
      ],
      "metadata": {
        "id": "TlzZZhLPKuRW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df[[\"Summary\",\"Text\"]]"
      ],
      "metadata": {
        "id": "g9t6l3TWOoo2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"train.csv\")"
      ],
      "metadata": {
        "id": "XNlCCatvVSkb"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_eval=df_eval[[\"Summary\",\"Text\"]]"
      ],
      "metadata": {
        "id": "CPCXjkgoNY3X"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_eval.to_csv(\"evaluate.csv\")"
      ],
      "metadata": {
        "id": "NlXxlpVXVUjy"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0][\"Text\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "id": "mGg8ObAAP_wa",
        "outputId": "53d059c8-fbf5-443e-e419-8fc7651b6805"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.iloc[0][\"Summary\"]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "CNO1gEsXIPen",
        "outputId": "7ff187ec-45e6-48c0-ec7b-3d8fe0ab364f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Good Quality Dog Food'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_eval.iloc[0][\"Text\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HcFg3WPnTBm4",
        "outputId": "5d7ce3be-bcf9-448a-8de0-e6bce3793f4b"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available, using CPU\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRfm2yCmIbNQ",
        "outputId": "646b9b25-67b4-4a4d-c02b-2e5fb764c713"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU is available\n",
            "Using GPU: Tesla T4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenize the Data"
      ],
      "metadata": {
        "id": "1plFKwC6NQmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
        "\n",
        "def tokenize_data(dataset):\n",
        "    '''\n",
        "    args: inpuut dataset, which is a dataframe contain two columns: Text, Summary\n",
        "    returns :\n",
        "    1.input_ids: This is a list (or tensor) of token indices that represent the tokenized input text. Each integer in this list corresponds to a specific token in the tokenizer's vocabulary. The input_ids are used by the model to look up the token embeddings during the forward pass.\n",
        "    2.attention_mask: This is a list (or tensor) of the same length as input_ids, containing 1s and 0s.\n",
        "    It is used to indicate which tokens should be attended to and which should be ignored. A value of 1 in the attention_mask means that the corresponding token in input_ids is a real token that should be attended to, while a value of 0 means that it is a padding token and should be ignored.\n",
        "    The attention_mask is important because it allows the model to handle variable-length input sequences.\n",
        "    The model can ignore padding tokens and focus only on the meaningful parts of the input.\n",
        "    The final output is a list of dictionaries, each dictionary contians the input_ids, attention_mask and labels\n",
        "    '''\n",
        "    tokenized_data = []\n",
        "    for index in range(len(dataset)):\n",
        "        text_encodings = tokenizer(dataset.iloc[index][\"Text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "        summary_encodings = tokenizer(dataset.iloc[index][\"Summary\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "        tokenized_data.append({\"input_ids\": torch.tensor(text_encodings[\"input_ids\"]), \"attention_mask\": torch.tensor(text_encodings[\"attention_mask\"]), \"labels\": torch.tensor(summary_encodings[\"input_ids\"])})\n",
        "    return tokenized_data\n"
      ],
      "metadata": {
        "id": "x_wS0hzCNP7y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76af827c-576d-47a0-a71e-301766226bdf"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = tokenize_data(df)"
      ],
      "metadata": {
        "id": "lYw_noW-RtPG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Create a Dataloader"
      ],
      "metadata": {
        "id": "mATXhcGbT6s3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SummarizationDataset(Dataset):\n",
        "    '''\n",
        "    args: Dataset, which is the tokenized dataset, a list contain dictionaries\n",
        "    returns: a pytorch Dataset, where each element is a dictionary from the tokenized dataset\n",
        "    '''\n",
        "    def __init__(self, data):\n",
        "        self.data=data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ],
      "metadata": {
        "id": "PPZdeqpLUDMQ"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SummarizationDataset(tokenized_dataset)\n",
        "#each batch in the dataloader is a dictionary, contain \"input_ids\",\"attention_mask\" and \"labels\" (each of hem is a 32*512 tensor)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "MK7FWJPKUJ2b"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i,batch in enumerate(train_dataloader):\n",
        "  if i==0:\n",
        "    batch=batch\n",
        "    break"
      ],
      "metadata": {
        "id": "ZmbsxJlkUkkD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch[\"input_ids\"].shape #example of the first batch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FV1_b7JkZx8Y",
        "outputId": "77987f6a-7165-4cdc-a3f3-0dc93c1875f8"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([10, 512])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Initialize the model"
      ],
      "metadata": {
        "id": "9Yt5tgeeGYMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")"
      ],
      "metadata": {
        "id": "hee09NeraHi-"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eMUt1Mg3JD0G",
        "outputId": "7b7380cb-21ab-43a8-bedb-52322e73859c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BartForConditionalGeneration(\n",
              "  (model): BartModel(\n",
              "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
              "    (encoder): BartEncoder(\n",
              "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BartEncoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (activation_fn): GELUActivation()\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): BartDecoder(\n",
              "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
              "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
              "      (layers): ModuleList(\n",
              "        (0-11): 12 x BartDecoderLayer(\n",
              "          (self_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (activation_fn): GELUActivation()\n",
              "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (encoder_attn): BartAttention(\n",
              "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "          )\n",
              "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Set up the training loop"
      ],
      "metadata": {
        "id": "mSlOWtSJGvDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "\n",
        "num_epochs = 1\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for batch in train_dataloader:\n",
        "        input_ids = batch[\"input_ids\"]\n",
        "        attention_mask = batch[\"attention_mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtqXvK9rGxD8",
        "outputId": "6af4758b-3db1-4b05-8d01-0b28c76bea64"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Loss: 12.198269844055176\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the model"
      ],
      "metadata": {
        "id": "W3R5TU-ZNg1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#generate summary for the texts in the evaluation dataset\n",
        "#pre-train model does not change the tokenizer\n",
        "\n",
        "def tokenize_data_eval(dataset):\n",
        "    tokenized_data = []\n",
        "    for index in range(len(dataset)):\n",
        "        text_encodings = tokenizer(dataset.iloc[index][\"Text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "        input_ids=torch.tensor(text_encodings[\"input_ids\"]).to(device)\n",
        "        attention_mask=torch.tensor(text_encodings[\"attention_mask\"]).to(device)\n",
        "        tokenized_data.append({\"input_ids\": input_ids , \"attention_mask\":attention_mask })\n",
        "    return tokenized_data"
      ],
      "metadata": {
        "id": "v0cKciG_NiyQ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset_eval=tokenize_data_eval(df_eval)"
      ],
      "metadata": {
        "id": "umNdqNJmOlw8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate(eval_tokenized):\n",
        "  summary_lis=[]\n",
        "  for token in eval_tokenized:\n",
        "\n",
        "    summary_ids = model.generate(token[\"input_ids\"].unsqueeze(0),attention_mask=token[\"attention_mask\"].unsqueeze(0), num_beams=4, max_length=60, early_stopping=True)\n",
        "\n",
        "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "    summary_lis.append(summary)\n",
        "  return summary_lis"
      ],
      "metadata": {
        "id": "O9jAQAICOtLR"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary=generate(tokenized_dataset_eval)"
      ],
      "metadata": {
        "id": "BYSlUvGrSnVO"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(df_eval)):\n",
        "  print(\"original text: \", df_eval.iloc[i][\"Text\"])\n",
        "  print(\"generated summary: \", summary[i] )\n",
        "  print(\"**********\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BKQwXjrSXJU",
        "outputId": "f6dc34ae-ab52-4459-b4b4-375e10fd8671"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original text:  I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind!  We picked up a bottle once on a trip we were on and brought it back home with us and were totally blown away!  When we realized that we simply couldn't find it anywhere in our city we were bummed.<br /><br />Now, because of the magic of the internet, we have a case of the sauce and are ecstatic because of it.<br /><br />If you love hot sauce..I mean really love hot sauce, but don't want a sauce that tastelessly burns your throat, grab a bottle of Tequila Picante Gourmet de Inclan.  Just realize that once you taste it, you will never want to use any other sauce.<br /><br />Thank you for the personal, incredible service!\n",
            "generated summary:  I don't know if it's the cactus or the tequila or just the unique combination of ingredients, but the flavour of this hot sauce makes it one of a kind! We picked up a bottle once on a trip we were on and brought it back home with us and were\n",
            "**********\n",
            "original text:  One of my boys needed to lose some weight and the other didn't.  I put this food on the floor for the chubby guy, and the protein-rich, no by-product food up higher where only my skinny boy can jump.  The higher food sits going stale.  They both really go for this food.  And my chubby boy has been losing about an ounce a week.\n",
            "generated summary:  One of my boys needed to lose some weight and the other didn't. I put this food on the floor for the chubby guy, and the protein-rich, no by-product food up higher where only my skinny boy can jump. The higher food sits going stale. They\n",
            "**********\n",
            "original text:  My cats have been happily eating Felidae Platinum for more than two years. I just got a new bag and the shape of the food is different. They tried the new food when I first put it in their bowls and now the bowls sit full and the kitties will not touch the food. I've noticed similar reviews related to formula changes in the past. Unfortunately, I now need to find a new food that my cats will eat.\n",
            "generated summary:  My cats have been happily eating Felidae Platinum for more than two years. I just got a new bag and the shape of the food is different. They tried the new food when I first put it in their bowls and now the bowls sit full and the kitties will not touch\n",
            "**********\n",
            "original text:  good flavor! these came securely packed... they were fresh and delicious! i love these Twizzlers!\n",
            "generated summary:  good flavor! these came securely packed... they were fresh and delicious! i love these Twizzlers! I love the smell of the Twizzler. I like the taste of the caramel. I love that they come in a pack of six. I also love the flavor of the\n",
            "**********\n",
            "original text:  The Strawberry Twizzlers are my guilty pleasure - yummy. Six pounds will be around for a while with my son and I.\n",
            "generated summary:  The Strawberry Twizzlers are my guilty pleasure - yummy. Six pounds will be around for a while with my son and I. We will be spending some time with our son and his family in the next few weeks. It will be great to have some time together.\n",
            "**********\n",
            "original text:  My daughter loves twizzlers and this shipment of six pounds really hit the spot. It's exactly what you would expect...six packages of strawberry twizzlers.\n",
            "generated summary:  Six packages of strawberry twizzlers are delivered to your home. The shipment of six pounds really hit the spot for your daughter. She loves twizzler's and this shipment was just what she needed. It's exactly what you would expect...six packages of Strawberry Twizzlers.\n",
            "**********\n",
            "original text:  I love eating them and they are good for watching TV and looking at movies! It is not too sweet. I like to transfer them to a zip lock baggie so they stay fresh so I can take my time eating them.\n",
            "generated summary:  I love eating them and they are good for watching TV and looking at movies! It is not too sweet. I like to transfer them to a zip lock baggie so they stay fresh so I can take my time eating them. It is good to look at movies but not too\n",
            "**********\n",
            "original text:  I am very satisfied with my Twizzler purchase.  I shared these with others and we have all enjoyed them.  I will definitely be ordering more.\n",
            "generated summary:  I am very satisfied with my Twizzler purchase. I shared these with others and we all enjoyed them. I will definitely be ordering more of these in the future. I am very happy with my purchase and will be ordering many more of them in the near future.\n",
            "**********\n",
            "original text:  Twizzlers, Strawberry my childhood favorite candy, made in Lancaster Pennsylvania by Y & S Candies, Inc. one of the oldest confectionery Firms in the United States, now a Subsidiary of the Hershey Company, the Company was established in 1845 as Young and Smylie, they also make Apple Licorice Twists, Green Color and Blue Raspberry Licorice Twists, I like them all<br /><br />I keep it in a dry cool place because is not recommended it to put it in the fridge. According to the Guinness Book of Records, the longest Licorice Twist ever made measured 1.200 Feet (370 M) and weighted 100 Pounds (45 Kg) and was made by Y & S Candies, Inc. This Record-Breaking Twist became a Guinness World Record on July 19, 1998. This Product is Kosher! Thank You\n",
            "generated summary:  Twizzlers, Strawberry my childhood favorite candy, made in Lancaster Pennsylvania by Y & S Candies, Inc. One of the oldest confectionery Firms in the United States, now a Subsidiary of the Hershey Company. According to the Guinness Book of Records, the\n",
            "**********\n",
            "original text:  Candy was delivered very fast and was purchased at a reasonable price.  I was home bound and unable to get to a store so this was perfect for me.\n",
            "generated summary:  Candy was delivered very fast and was purchased at a reasonable price. I was home bound and unable to get to a store so this was perfect for me.  I was very pleased with the service and the price was very reasonable. I will be using this for the rest of my\n",
            "**********\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"summary.txt\",\"w\") as f:\n",
        "  for summar in summary:\n",
        "    f.write(summar+\"/n\")"
      ],
      "metadata": {
        "id": "PPewFX7MUthP"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#use rouge_score to measure the overlap between the generated summary and the reference summary in terms of n-grams, word sequences, and word pairs\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Assuming you have a list of reference summaries and generated summaries\n",
        "reference_summaries = list(df_eval[\"Text\"])\n",
        "generated_summaries = summary\n",
        "scores = [scorer.score(ref, gen) for ref, gen in zip(reference_summaries, generated_summaries)]\n",
        "\n",
        "# Calculate average scores\n",
        "avg_scores = {key: sum(score[key].fmeasure for score in scores) / len(scores) for key in scores[0].keys()}\n",
        "print(avg_scores)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t1uz_Qb8WItp",
        "outputId": "342594ec-ee5d-447d-92a1-07faa9cd2ef1"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'rouge1': 0.6680995380365928, 'rouge2': 0.6481616838899383, 'rougeL': 0.6569884269254817}\n"
          ]
        }
      ]
    }
  ]
}